<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Random Forest vs XGBoost Visualization</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background-color: #f5f5f5;
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }

        .buttons {
            display: flex;
            justify-content: center;
            margin-bottom: 20px;
        }

        button {
            padding: 10px 20px;
            margin: 0 10px;
            border: none;
            border-radius: 5px;
            background-color: #ccc;
            cursor: pointer;
            font-size: 16px;
        }

        button.active {
            background-color: #3498db;
            color: white;
        }

        #visualization-container {
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            min-height: 450px;
        }

        .comparison, .pros-cons, .use-cases {
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }

        .comparison h3, .pros-cons h3, .use-cases h3 {
            margin-top: 0;
            margin-bottom: 15px;
        }

        .comparison-grid, .pros-cons-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
        }

        .rf-column {
            border-right: 1px solid #ddd;
            padding-right: 20px;
        }

        .xgb-column {
            padding-left: 20px;
        }

        h4 {
            margin-top: 0;
        }

        .rf-column h4 {
            color: #3498db;
        }

        .xgb-column h4 {
            color: #2ecc71;
        }

        ul {
            padding-left: 20px;
        }

        li {
            margin-bottom: 5px;
            font-size: 14px;
        }

        .tooltip-group {
            cursor: help;
        }

        .tooltip {
            pointer-events: none;
        }
        
        .pros {
            color: #27ae60;
            font-weight: bold;
        }
        
        .cons {
            color: #e74c3c;
            font-weight: bold;
        }
        
        .use-case {
            background-color: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 10px 15px;
            margin-bottom: 10px;
            border-radius: 0 5px 5px 0;
        }
        
        .use-case.xgb {
            border-left-color: #2ecc71;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="buttons">
            <button id="randomForestBtn" class="active">Random Forest</button>
            <button id="xgboostBtn">XGBoost</button>
        </div>
        
        <div id="visualization-container"></div>
        
        <div class="comparison">
            <h3>Key Differences:</h3>
            <div class="comparison-grid">
                <div class="rf-column">
                    <h4>Random Forest</h4>
                    <ul>
                        <li><b>Parallel:</b> Trees are trained independently (faster training)</li>
                        <li><b>Bagging:</b> Each tree uses random subset of data (reduces variance)</li>
                        <li><b>Equal weights:</b> All trees contribute equally to the final prediction</li>
                        <li><b>Feature sampling:</b> Each tree trains on a random subset of features</li>
                        <li><b> Overfitting Prevention:</b> Works well by reducing variance through ensembling</li>
                        <li><b>Final decision:</b> Majority voting (classification) or averaging (regression)</li>
                    </ul>
                </div>
                <div class="xgb-column">
                    <h4>XGBoost</h4>
                    <ul>
                        <li><b>Sequential (Boosting):</b>  Trees are built iteratively, improving previous errors</li>
                        <li><b>Error-focused:</b> Each tree corrects the mistakes of the previous ones</li>
                        <li><b>Weighted:</b> Trees contribute differently based on their learning importance</li>
                        <li><b>Regularization:</b> Uses L1 and L2 regularization to control complexity</li>
                        <li><b>Gradient-based:</b> Optimized using gradient boosting (gradient descent-like updates)</li>
                        <li><b>Efficient Parallelization:</b> Although boosting is sequential, XGBoost parallelizes split finding and feature selection</li>
                        <li><b>Final decision:</b> Weighted sum of all tree predictions</li>
                    </ul>
                </div>
            </div>
        </div>
        
        <div class="pros-cons">
          <h3>Pros and Cons:</h3>
          <div class="pros-cons-grid">
              <div class="rf-column">
                  <h4>Random Forest</h4>
                  <ul>
                      <li><span class="pros">PRO:</span> Easy to tune (fewer hyperparameters)</li>
                      <li><span class="pros">PRO:</span> Less prone to overfitting</li>
                      <li><span class="pros">PRO:</span> Works well with high-dimensional and tabular data</li>
                      <li><span class="pros">PRO:</span> Handles missing values naturally (can split on missing values)</li>
                      <li><span class="pros">PRO:</span> More interpretable (stable feature importance rankings)</li>
                      <li><span class="cons">CON:</span> Can be slower for very large datasets</li>
                      <li><span class="cons">CON:</span> May not perform well on highly imbalanced datasets</li>
                      <li><span class="cons">CON:</span> Can struggle with complex feature interactions compared to boosting</li>
                  </ul>
              </div>
              <div class="xgb-column">
                  <h4>XGBoost</h4>
                  <ul>
                      <li><span class="pros">PRO:</span>  Often achieves higher predictive accuracy than Random Forest</li>
                      <li><span class="pros">PRO:</span> Built-in regularization (L1 & L2) reduces overfitting risks</li>
                      <li><span class="pros">PRO:</span>  Efficient for sparse datasets (e.g., text data)</li>
                      <li><span class="pros">PRO:</span> Highly optimized computation (parallelized split finding)</li>
                      <li><span class="pros">PRO:</span> Handles missing values without imputation</li>
                      <li><span class="cons">CON:</span> More hyperparameters â†’ Requires careful tuning</li>
                      <li><span class="cons">CON:</span> Can overfit if not properly regularized</li>
                      <li><span class="cons">CON:</span>  Training is sequential in boosting, so it may be slower on very large datasets</li>
                  </ul>
              </div>
          </div>
      </div>
      
      <div class="use-cases">
          <h3>When to Use:</h3>
          
          <h4>Random Forest</h4>
          <div class="use-case">
              <p><strong>Tabular data with many features:</strong> When dealing with high-dimensional datasets with many features but limited samples, Random Forest's feature randomness helps prevent overfitting.</p>
          </div>
          <div class="use-case">
              <p><strong>When interpretability matters:</strong> Feature importance is more stable and intuitive in Random Forest, making it better for applications where understanding feature contributions is critical.</p>
          </div>
          <div class="use-case">
              <p><strong>Limited hyperparameter tuning resources:</strong> When you need a model that works well out-of-the-box without extensive parameter tuning.</p>
          </div>
          <div class="use-case">
            <p><strong>When training speed is a concern:</strong> Since trees train in parallel, it is faster than XGBoost on large datasets.</p>
          </div>
          
          <h4 style="color: #2ecc71; margin-top: 20px;">XGBoost</h4>
          <div class="use-case xgb">
              <p><strong>Maximizing predictive performance:</strong>  Best for scenarios where predictive performance is the top priority.</p>
          </div>
          <div class="use-case xgb">
              <p><strong>Imbalanced datasets:</strong> XGBoost's sequential error correction approach often handles class imbalance better than Random Forest.</p>
          </div>
          <div class="use-case xgb">
              <p><strong>Structured data with complex relationships:</strong> When the relationships between features are intricate and non-linear, XGBoost's additive model can capture these interactions more effectively.</p>
          </div>
      </div>
    <!-- d3 import -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- import javascript -->
    <script src="script.js"></script>
</body>
</html>